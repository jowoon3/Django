<!DOCTYPE html>
{% load static %}
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>파이썬 웹스크래핑</title>
  <script src="https://code.jquery.com/jquery-3.4.1.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha2/dist/js/bootstrap.bundle.min.js"></script>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">   
  <link href="https://getbootstrap.com/docs/5.3/assets/css/docs.css" rel="stylesheet">     
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.4/font/bootstrap-icons.css">
    <!-- font css -->
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;800&display=swap" rel="stylesheet">
  <link href="{% static '/css/java.css'%}" rel="stylesheet">   
  <link href="{% static '/css/style.css'%}" rel="stylesheet">     
</head>
<body class="ms-3">  
  <H2 class="coffee_taital" style="text-align:left;"><a name="home">파이썬</a></H2>  
    <p><a href="/python/p20.html" class="btn btn-warning mb-1" target="_blank">이전 노트</a>
      <a href="/python/p21.html" class="btn btn-warning mb-1" target="_blank">다음 노트</a>            
      <a href="/htmls/tut1.html" class="btn btn-danger mb-1" target="_blank">HTML 노트</a> 
      <a href="/css/tut11.html" class="btn btn-success mb-1" target="_blank">CSS 노트</a> 
      <a href="/javascript/j01.html" class="btn btn-purple mb-1" target="_blank">Javascript 노트</a>
     <a href="/python/p01.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">파이썬의 기초</a>
     <a href="/python/p02.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">변수</a>
     <a href="/python/p03.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">숫자형타입</a>
     <a href="/python/p04.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">불리언타입</a>
     <a href="/python/p05.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">문자열타입</a>
     <a href="/python/p06.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">조건문</a>
     <a href="/python/p07.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">반복문</a>
     <a href="/python/p08.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">break와 continue</a>
     <a href="/python/p09.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">리스트 타입</a>
     <a href="/python/p10.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">튜플 타입</a>
     <a href="/python/p11.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">세트 타입</a>
     <a href="/python/p12.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">딕셔녀리 타입</a>
     <a href="/python/p13.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">함수</a>
     <a href="/python/p14.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">변수의 유효범위</a>
     <a href="/python/p15.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">클래스</a>
     <a href="/python/p16.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">객체</a>
     <a href="/python/p17.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">상속</a>
     <a href="/python/p18.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">모듈</a>
     <a href="/python/p19.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">파일 입출력</a>
     <a href="/python/p20.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">예외처리</a>      
     <a href="/python/p21.html" class=" btn btn-secondary mb-1" target="_blank">웹스크래핑1</a>     
     <a href="/python/p22.html" class=" btn btn-outline-warning border-2 mb-1" target="_blank">웹스크래핑2</a>   </p> 


  <div class="col-md-8 mb-2">
  <div class="p-3 text-white fs-4 bd-pink-800">html 문서 웹스크래핑</div>
  <p>출처 : https://docs.python.org/ko/3/library/ <br>
  https://docs.python.org/ko/3/library/urllib.parse.html#url-parsing 파이썬 URL 구문 분석<br>
  의외로 파이썬의 url 문법을 잘 알아야 웹스크래핑이 가능하다 <br><br>
  <span class="fw-bold">&lt;urllib></span>은  URL 작업을 위한 여러 모듈을 모은 패키지 <br>
** urllib.request : URL을 열고 읽기 위한 모듈 <br>
** urllib.response : URL 반응을 읽기 위한 모듈. urllib.request에 의해 호출됨 <br>
** urllib.error : urllib.request에 의해 발생하는 예외를 포함하는  모듈 <br>
** urllib.parse : URL 구문 분석을 위한  모듈 <br>
** urllib.robotparser : robots.txt 파일을 구문 분석하기 위한 모듈 <br>

<hr>
<h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">URL을 열고 읽기</h2>
<p>urllib.request 모듈은 urllib.request.urlopen과 urllib.request.request 등의 함수를 제공한다. <br>
  import 함수명 from urllib.request 으로 함수를 호출하고 코드에서는 함수명()으로 사용하거나 <br> 
  import urllib.request 하고 코드에서는 urllib.request.함수명() 으로 사용한다 <br><br> 
  urllib.request.urlopen은 URL을 열고 해당 URL에서 데이터를 읽어오는 함수이고<br>
  urllib.request.request는 HTTP 요청을 만들기 위해 사용되며 HTTP 메서드(GET, POST 등)와 함께 URL을 지정하고 요청 헤더 등을 설정할 수 있다.<br>
  urllib.request.urlretrieve는 URL로 표시된 네트워크 객체를 로컬 파일로 복사한다 <br>
  <br>
<span class="fw-bold">  urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) <br> 
urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)<br>
urllib.request.urlretrieve(url, filename=None, reporthook=None, data=None)</span>
  <p class="bd-yellow-100 p-3 ms-3"> 
    -------------------- urlopen --------------------<br>
  import urllib.request<br>
  response = urllib.request.urlopen('https://www.example.com/')<br>
  html = response.read()<br>
  print(html)<br>
  -------------------- request --------------------<br>
  import urllib.request<br>
  import urllib.parse<br>
  url = 'http://www.example.com/search'<br>
  values = {'q': 'python programming'}<br><br>

  data = urllib.parse.urlencode(values).encode('utf-8')<br>
  req = urllib.request.Request(url, data)<br>
  response1 = urllib.request.urlopen(req)<br>
  html1 = response.read()<br>
  print(html1)<br>
  -------------------- urlretrieve --------------------<br>
  import urllib.request <br>
url = "https://t1.daumcdn.net/daumtop_chanel/op/20200723055344399.png"  # 경로지정<br>
imgName="C:\pySrc\daum.png"  # 이름 지정<br><br>

urllib.request.urlretrieve(url, imgName)  // urlretrieve(경로URL, 저장할 파일경로) 다운로드<br>
print("다운로드 완료되었습니다")<br><br>
</p>

  <hr>
  <h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">URL </h2>
  <p><span class="fw-bold">  from urllib.parse import urlparse <br>
  urlparse("scheme://netloc/path;parameters?query#fragment")</p></span>
  <p class="bd-yellow-100 p-3 ms-3"> 
  o = urlparse("http://docs.python.org:80/3/library/urllib.parse.html?highlight=params#url-parsing") <br>
  o.scheme => 'http' // 주소체계 <br> 
  o.netloc => 'docs.python.org:80' //네트워크위치. 위에서 sheme// 으로 시작해야 netloc으로 인식됨.  <br> 
  o.path => '3/library/urllib.parse.html' //경로 
  o.hostname=> 'docs.python.org' <br>
  o.port =>  80 <br>
  o.query  => 'highlight=params'<br>
  o.fragment => 'url-parsing'<br>
  o._replace(fragment="").geturl() <br></p> 

  <hr>
  <h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">html파일 호출</h2>
  <p><span class="fw-bold">1. requests 모듈 설치</p></span>
  <p class="bd-yellow-100 p-3 ms-3">pip install requests</p>

  <p><span class="fw-bold">2. URL 요청하기 - get</p></span>
  <p class="bd-yellow-100 p-3 ms-3">
import requests<br>
response = <span class="text-danger">requests.get('https://www.naver.com/')</span><br>//urlopen(('https://www.naver.com/')과 비슷한 뜻<br>
print(response.status_code)<br>
print(response.text)<br></p>
<p>status_code 는 응답코드를 가져온다. text에는 HTML 코드가 담겨 있다.</p>

<p><span class="fw-bold">파라미터를 전달하는 방법</p></span>
<p class="bd-yellow-100 p-3 ms-3">
import requests<br>
url = 'https://section.blog.naver.com/Search/Post.nhn?pageNo=1&rangeType=ALL&orderBy=sim&keyword=%ED%8C%8C%EC%9D%B4%EC%8D%AC'<br><br>
params = {<br>
    'pageNo' : 1,<br>
    'rangeType' : 'ALL',<br>
    'orderBy' : 'sim',<br>
    'keyword' : '파이썬'<br>
}<br><br>
response = requests.get('https://section.blog.naver.com/Search/Post.nhn', params=params)<br><br>
print(response.status_code)<br>
print(response.url)<br></p>

<p>여기서 받아오는 것은 텍스트형태의 html 이다. 텍스트형태의 데이터에서 원하는 html 태그를 추출하기 위해 뷰티풀수프나 셀레니움등이 사용된다</p> </div>

<div class="col-md-8 mb-2">
  <div class="p-3 text-white fs-4 bd-pink-800">웹크롤러 (웹 스크래퍼)사용하기</div>
  
  <hr>
  <h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">BeautifulSoup 설치</h2>

  <p><span class="fw-bold">BeautifulSoup 설치</span></p>
  <p class="bd-yellow-100 p-3 ms-3">
pip install beautifulsoup4<br></p>

<hr>
<h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">BeautifulSoup 사용</h2>
<p><span class="fw-bold">BeautifulSoup 사용법</span></p>
<p>문법</p>
<p><span class="fw-bold">BeautifulShop(HTML 텍스트, 구문분석기)<br></span>
** HTML 텍스트 : HTML markup, html 또는 html.read()<br>
** 구문분석기(parser) : 'html.parser', 'lxml', lxml-xml', 'xml', 'html5lib' <br></p>

<p><span class="fw-bold">BeautifulSoup 수프 만들기</span></p>
<p>문서를 해석하려면, 문서의 문자열 혹은 열린 파일 핸들을 BeautifulSoup constructor(구성자)에 건네면 된다:<br>
  <p class="bd-yellow-100 p-3 ms-3">
    from bs4 import BeautifulSoup<br></p>
    soup = BeautifulSoup(open("index.html"))<br>
    soup = BeautifulSoup("<html>data</html>")<br>
    <br>//먼저, 문서는 유니코드로 변환되고 HTML 개체는 유니코드 문자로 변환된다    <br> 
BeautifulSoup("Sacr&amp;eacute; bleu!")<br> 
&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Sacré bleu!&lt;/body&gt;&lt;/html&gt; <br> </p>
<p>다음 뷰티플수프는 문서에 가장 적당한 구문 분석기를 사용하여 해석한다. 특별히 XML 해석기를 사용하라고 지정해 주지 않으면 HTML 해석기를 사용한다. <br> 
  <p class="bd-yellow-100 p-3 ms-3">
    import requests<br>
    from bs4 import BeautifulSoup<br><br>
    url = 'https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC' // 네이버 지식인에 파이썬을 검색한 url <br>
    response = requests.get(url)<br><br>
    
    if response.status_code == 200: //응답 코드가 200 일때<br>  
        html = response.text // html 을 받아와<br>
        soup = BeautifulSoup(html, 'html.parser') // soup 객체로 변환<br>
        print(soup)<br><br>
    
    else : <br>
        print(response.status_code)<br></p>
    
    <p>특정 HTML 페이지에서 원하는 제목을 가져오려면 구글 개발자도구를 통해 원하는 html 요소를 찾는다.<br>
      찾은 html에 오른쪽 클릭을 한 후 Copy -> Copy Selector 를 선택하면 클립보드에 복사된다.</p> 
    
      <p class="bd-yellow-100 p-3 ms-3"> 
    import requests<br>
    from bs4 import BeautifulSoup<br><br>
    
    url = 'https://kin.naver.com/search/list.nhn?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'<br>
    response = requests.get(url)<br><br>
    
    if response.status_code == 200:<br>
        html = response.text<br>
        soup = BeautifulSoup(html, 'html.parser')<br>
        title = soup.select_one('#s_content > div.section > ul > li:nth-child(1) > dl > dt > a')<br>
        //select_one 은 하나의 html 요소를 찾는 함수인데, 복사한 css 선택자를 select_one 함수의 인자로 넣어준다<br>
        print(title)<br>
        print(title.get_text()) // 텍스트만 뽑아오고 싶다면 결과: 이거 <span class="fw-bold">파이썬 알려주세룜..<br></span>
    else : <br>
        print(response.status_code)</p>
        <p class="bd-blue-100 p-3 ms-3"> 
    <span class="fw-bold">결과</span>
    &lt;a class="_nclicks:kin.txt _searchListTitleAnchor" href="https://kin.naver.com/qna/detail.naver?d1id=1&amp;dirId=10402&amp;docId=445100913&amp;qb=7YyM7J207I2s&amp;enc=utf8§ion=kin&amp;rank=1&amp;search_sort=0&amp;spq=0" target="_blank">이거 &lt;b>파이썬&lt;/b> 알려주세룜..&lt;/a>
    </p> 
<hr>
<p><span class="fw-bold">BeautifulSoup 수프 객체 종류</p></span>
<p>뷰티플수프는 복합적인 HTML 문서를 파이썬 객체로 구성된 복합적인 문서로 변환한다.  <br> <span class="fw-bold">
  1. 태그 객체 <br>
  &nbsp;&nbsp;&nbsp;1.1 이름<br>
  &nbsp;&nbsp;&nbsp;1.2 속성 <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2.1  값이-여럿인 속성 <br>
  2. NavigableString 객체<br>
  3. BeautifulSoup 객체<br>
  4. Comment 객체<br>
  <br></span>
  <span class="fw-bold">1. 태그 : </span>태그에는 많은 속성과 메쏘드가 있지만, 가장 중요한 특징인 이름과 속성을 설명한다.<br>
  <span class="fw-bold">1.1 이름 : </span>태그마다 이름이 있고, .name 과 같이 접근할 수 있다<br>
  <span class="fw-bold">1.2 속성 : </span>태그는 여러개 속성을 가질 수 있다. 사전처럼 속성과 태그를 반복해 접근하면 된다<br><br> 
  <p class="bd-yellow-100 p-3 ms-3"> 
    soup = BeautifulSoup('&lt;b class="boldest">Extremely bold&lt;/b>')<br>
    tag = soup.b<br>
    type(tag)   # &lt;class 'bs4.element.Tag'><br><br>
    tag.name   # 'b'  // 태그이름<br>
    tag.name = "blockquote" //태그의 이름을 바꾸면, 그 변화는 뷰티블수프가 생산한 HTML 조판에 반영된다 <br>
    tag    # &lt;blockquote class="boldest">Extremely bold&lt;/blockquote><br><br>    
    tag['class']  # ['boldest'] // 태그속성. class는 객체나 리스트로 저장가능.(여러개 속성가능하므로) 참고로 tag.class는 오류발생한다 <br>
    tag['class'] =['pig', 'dog', 'duck'] // 기존 클래스를 덮어씌운다
    <br>
    tag    # &lt;blockquote class="pig dog duck">Extremely bold&lt;/blockquote><br><br> 
    tag= BeautifulSoup('&lt;p class="body">&lt;/p>')<br>
    css_soup.p['class']  # ["body"]<br><br> 
    tag.attrs    # {'class': ['boldest']}  사전의 .attrs와 같이 접근할 수 있다 <br><br>    
    tag['class'] = 'verybold'  //태그의 속성을 추가, 제거, 변경할 수 있다. 단 기존의 태그를 덮어씌우는 개념이다<br>
    tag['id'] = 1		   //태그를 사전처럼 취급해서 처리한다.<br>
    tag       # &lt;blockquote class="verybold" id="1">Extremely bold&lt;/blockquote><br><br>    
    del tag['class']			//태그의 속성을 제거<br>
    del tag['id']<br>
    tag       # &lt;blockquote>Extremely bold&lt;/blockquote><br><br>
    tag['class']   # KeyError: 'class'<br>
    print(tag.get('class'))    # None<br></p>

    
  <p><span class="fw-bold">1.2.1 값이-여럿인 속성: </span>HTML5에서 몇몇 속성은 값을 여러 개 가질 수 있도록 정의된다.<br>
  가장 흔한 다중값 속성은 class이다 (다시 말해, 태그가 하나 이상의 CSS 클래스를 가질 수 있다). <br>
  그 외 rel, rev, accept-charset, headers, 그리고 accesskey도 여러개 값을 가질수 있다. <br>
   뷰티플수프는 다중-값 속성의 값들을 리스트로 나타낸다<br><br></p>
   <p class="bd-yellow-100 p-3 ms-3"> 
    css_soup = BeautifulSoup('&lt;p class="body strikeout">&lt;/p>')<br>
    css_soup.p['class']  # ["body", "strikeout"]<br><br>    
    css_soup = BeautifulSoup('&lt;p class="body">&lt;/p>')<br>
    css_soup.p['class']  # ["body"]<br><br>

    id_soup = BeautifulSoup('&lt;p id="my id">&lt;/p>') //속성에 하나 이상의 값이 있는 것처럼 보이지만, <br>
    id_soup.p['id']   # 'my id'   // HTML 표준에 정의된 다중-값 속성이 아니면, 뷰티플수프는 그 속성을 그대로 둔다 (리스트로 반환하지 않는다)<br><br>    
    rel_soup = BeautifulSoup('&lt;p>Back to the &lt;a rel="index">homepage&lt;/a>&lt;/p>')<br>
    rel_soup.a['rel']  # ['index']<br>
    rel_soup.a['rel'] = ['index', 'contents']<br>
    print(rel_soup.p)   # &lt;p>Back to the &lt;a rel="index contents">homepage&lt;/a>&lt;/p> //태그를 다시 문자열로 바꾸면, 다중-값 속성은 합병된다<br><br>
    
    xml_soup = BeautifulSoup('&lt;p class="body strikeout">&lt;/p>', 'xml') //문서를 XML로 해석하면, 다중-값 속성은 없다<br>
    xml_soup.p['class']  # u'body strikeout'<br></p>

    
  <p><span class="fw-bold">2. NavigableString 객체<br></span>
태그 안에 있는 일련의 텍스트에 상응하는 문자열들을 뷰티플수프는 NavigableString 클래스 안에 보관한다<br>
NavigableString은 파이썬의 유니코드 문자열과 똑같은데, 트리 탐색하기에 기술된 특징들도 지원한다. <br>
태그는 다른 문자열이나 또다른 태그가 담길 수 있는데 비해 문자열에는 다른 어떤 것도 담길 수 없으므로, <br> 
문자열은 .contents나 .string 속성, 또는 find() 메쏘드를 지원하지 않는다.<br><br> </p>
<p class="bd-yellow-100 p-3 ms-3"> 
tag.string  # u'Extremely bold'<br>
type(tag.string)  # &lt;class 'bs4.element.NavigableString'><br><br>
unicode_string = unicode(tag.string)  //NavigableString을 유니코드 문자열로 변환하려면 unicode()를 사용한다<br>
unicode_string   # u'Extremely bold'<br>
type(unicode_string)   # &lt;type 'unicode'><br> <br> 
tag.string.replace_with("No longer bold") //문자열을 바로바로 편집할 수는 없지만, replace_with()을 사용하여 다른 문자열로 바꿀 수 있다<br> 
tag    # &lt;blockquote>No longer bold&lt;/blockquote> <br> <br></p>

<span class="fw-bold">3. BeautifulSoup 객체<br> </span>
BeautifulSoup 객체 자신은 문서 전체를 대표한다. 대부분 Tag 객체로 취급해도 좋다. <br> 
그러나 실제 HTML 태그나 XML 태그에 상응하지 않기 때문에, 이름도 속성도 없다. <br> 
단, 특별히 .name에 "[document]"라는 이름이 주어졌다<br> 
<p class="bd-yellow-100 p-3 ms-3"> 
soup.name<br>
# u'[document]'<br> </p></p><br>

<p><span class="fw-bold">3. Comment 주석 객체<br> </span>
Comment 객체는 그냥 특별한 유형의 NavigableString이다. <br> 
그러나 실제 HTML 태그나 XML 태그에 상응하지 않기 때문에, 이름도 속성도 없다. <br> 
단, 특별히 .name에 "[document]"라는 이름이 주어졌다</p>
<br><p class="bd-yellow-100 p-3 ms-3"> 
  markup = "&lt;b>&lt;!--Hey, buddy. Want to buy a used parser?-->&lt;/b>"<br>
  soup = BeautifulSoup(markup)<br>
  comment = soup.b.string<br>
  type(comment)<br>
  # &lt;class 'bs4.element.Comment'> <br><br>
  # u'Hey, buddy. Want to buy a used parser'<br>
  그러나 HTML 문서의 일부에 나타나면, Comment는 특별한 형태로 화면에 표시된다<br><br>
  print(soup.b.prettify())<br>
</p>

<p class="bd-blue-100 p-3 ms-3"> 
  <span class="fw-bold">결과<br></span></span>
  # &lt;b> <Br>
  #  &lt;!--Hey, buddy. Want to buy a used parser?--><Br>
  # &lt;/b></p><br>   

<p> 뷰티플수프는 XML 문서에 나올만한 것들을 모두 클래스에다 정의한다<br>
  CData, ProcessingInstruction, Declaration, 그리고 Doctype도 Comment처럼 NavigableString의 하위클래스로서 자신의 문자열에 <br>
  다른 어떤것들을 추가한다 </p>   

  <br><p class="bd-yellow-100 p-3 ms-3">   
 주석을 CDATA 블록으로 교체하는 예 <br>
from bs4 import CData <Br>
cdata = CData("A CDATA block")<br>
comment.replace_with(cdata)<br></p>

<p class="bd-blue-100 p-3 ms-3"> 
<span class="fw-bold">결과<br></span></span>
print(soup.b.prettify())<br>
# &lt;b> <Br>
#  &lt;![CDATA[A CDATA block]]><br>
# &lt;/b></p><br>   

<hr>
<h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">뷰티플수프의 검색함수 : find()와 findAll</h2>

<p>문법</p>
<p><span class="fw-bold">findAll(tag, attributes, recursive, text, limit, keywords)<br>
  find(tag, attributes, recursive, text, limit, keywords)<br><br></span>
** tag : h1, h2 ..이런것들. 태그이름인 문자열을 넘기거나 태그이름으로 이루어진 <span class="text-danger">리스트를 반환한다</span><br>
** attributes: 속성으로 이루어진 파이썬 딕셔너리를 받고 그중 하나에 일치하는 태그를 찾는다. 예) bs.findAll('span', {'class':{'green', 'red'}}) <br>
** recursive: 얼마나 깊이 찾고자 하는지. True이면 태그 - 자식 - 자식의 자식...를 찾고 false이면 문서의 최상위태그만 찾는다 <br>
** text: 특정 텍스트가 몇번 나왔는지 찾을때 사용 예)  bs.findAll(text = 'the prince') <br>
** limit: findAll에만 사용된다. 검색한 것중 몇개까지 추출할것인지<br>
** keywords: 특정 속성이 포함된 태그를 찾을때<br><br>
실제로는 대부분 처음 두개 매개변수인 tag와 attributes만 사용한다<br>
조건 태그목록이 길어지면 and 필터가 아닌 or 필터로 작동해서 오히려 필요없는것들을 많이 선택하게 된다</p>
<br>

<p class="bd-blue-100 p-3 ms-3"> 
<span class="fw-bold">결과<br></span>
bs.findAll(id='text')<br>
bs.finaAll('', {'id':'text'})<br>
두줄이 같다</p><br>

<p class="bd-blue-100 p-3 ms-3"> 
  <span class="fw-bold">결과<br></span>
  bs.findAll(class='green')<br>
  bs.findAll(class_='green')<br>
  bs.finaAll('', {'class':'green'})<br>
첫째줄은 에러가 생기고 둘째 셋째줄은 같다</p><br> 
</p> 

<hr>
<h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">뷰티플트리 항해</h2>

<p><span class="fw-bold">자식, 자손 </p></span>
<p>자식만 찾을때는 .children()을 사용하고 자손을 찾을때는 .descendants()을 사용한다</p>
<p><span class="fw-bold">형제 </p></span>
<p>자기자신을 제외한 다음형제는 .next_sibilings()를 사용하고 이전형제는 .previous_siblings()를 사용한다. <br>
위는 리스트를 반환하지만 .next_sibiling(), .previous_sibling()은 태그 하나만 반환한다.</p>
<p><span class="fw-bold">부모 </p></span>
<p>자식과 반대개념으로 부모태그를 찾을때는 .parent(), .parents()를 사용한다</p><br>

<p class="bd-yellow-100 p-3 ms-3"> 
  from urllib.request import urlopen<br> 
  from bs4 import BeautifulSoup<br><br>
  
  html=urlopen('http://pythonscraping.com/pages/page3.html')<br>
  bs=BeautifulSoup(html.read(), 'html.parser')<br>
  -------------------- 자식 --------------------<br>
  for child in bs.find('table', {'id':'giftList'}).children: <br>
  &nbsp;&nbsp;&nbsp;&nbsp;print(child)<br>
  -------------------- 형제 --------------------<br>
  for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings: <br>
  &nbsp;&nbsp;&nbsp;&nbsp;print(sibling)<br> 
  -------------------- 부모 --------------------<br>
  print(bs.find('img', {'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.get_text()) <br>
  //get_text()는 태그를 제거해주는 함수 <br><br>  </p>
 
 <hr>
 <h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">뷰티플수프와 정규표현식</h2>
 <p><a href="/complete web develoment bootcamp/JS/j15.html#regex" target="_blank"><span class="fw-bold">정규표현식</span>에 대한 자세한 설명 참조</a><br>
스크래핑페이지의 레이아웃이 변경되거나 특정태그의 위치가 변경될 경우 원하는 태그가 노출되지 않을수 있다 <br>
이를 해결하기 위해 <span class="text-danger">정규표현식</span>을 이용하여 원하는 태그 자체를 식별하게 하는것이다  <br><br>

<p class="bd-yellow-100 p-3 ms-3"> 
  from urllib.request import urlopen<br> 
  from bs4 import BeautifulSoup<br>
  import re => 정규표현식 import<br>
  
  html=urlopen('http://pythonscraping.com/pages/page3.html')<br>
  bs=BeautifulSoup(html.read(), 'html.parser')<br> <br>
  
  regex = re.compile('\.\.\/img\/gifts\/img.*\.jpg')   =>../img/gifts/img문자1개이상.jpg 를 만족하는 텍스트 <br>
  images = bs.findAll('img', {'src':regex'} ) 
  for image in in images: <br>
  &nbsp;&nbsp;&nbsp;&nbsp;print(image['src'])<br>
<br><br></p> 

<hr>
<h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">뷰티플수프와 람다표현식</h2>
<p><a href="/python/p13.html#lambda" target="_blank"><span class="fw-bold">람다표현식과 map</span>에 대한 자세한 설명 참조</a><br>
스크래핑페이지의 레이아웃이 변경되거나 특정태그의 위치가 변경될 경우 원하는 태그가 노출되지 않을수 있다 <br>
이를 해결하기 위해 <span class="text-danger">정규표현식</span>을 이용하여 원하는 태그 자체를 식별하게 하는것이다  <br><br>

<p class="bd-yellow-100 p-3 ms-3"> 
 bs.findAll(lambda tag : len(tag.attrs)==2) // 매개변수로 넘기는 함수 len(tag.attrs)==2 가 true로 평가되면 findAll함수가 해당태그를 반환한다<br> <br> 
 bs.findAll(lambda tag : tag.get_text() = 'Or maybe he\'s only resting?') <br>

</p></p>
<hr>
<h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">웹스크래핑 에러와 에외처리</h2>
<p><span class="fw-bold">1. HTTP 에러 </p></span>
  <p>"404 Page Not found"나 "500 Internal Server Error" 등 페이지를 찾지 못하거나 URL 해석에서 에러가 생긴 경우</p>

<p><span class="fw-bold">2. URL 에러 </p></span>
  <p>서버를 찾을수 없거나 서버웹사이트 다운등으로 에러가 생긴 경우</p>

<p><span class="fw-bold">3. Tag 에러 </p></span>
  <p>실제 존재하지 않는 태그에 접근하면 일단 "None 객체"를 반환한다. 그러나 그 객체의 하위태그를 검색하면 Attribute Error가 생긴다<br>
    Attribute Error : 'NoneType' object has no attribute 'someTag'</p>
   
<p class="bd-yellow-100 p-3 ms-3"> 
  from urllib.request import urlopen<br>
  from urllib.error import HTTPError <br>
  from bs4 import BeautifulSoup<br><br>
  
  def getTitle(url):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;try:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;html=urlopen(url)<br>
  &nbsp;&nbsp;&nbsp;&nbsp;except HTTPError as e:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return None<br> 
  &nbsp;&nbsp;&nbsp;&nbsp;try:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bs=BeautifulSoup(html.read(), 'html.parser')<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title=bs.body.h1<br>
  &nbsp;&nbsp;&nbsp;&nbsp;except AttributeError as e:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return None //URLError는 HTTP에러나 Attribute에러에 걸리게 되어있다<br>
  &nbsp;&nbsp;&nbsp;&nbsp;return title<br><br>
  title = getTitle('http://pythonscraping.com/pages/page1.html')<br>
  if title == None<br>
  &nbsp;&nbsp;&nbsp;&nbsp;print('Title is not be found')<br>
  else:
  &nbsp;&nbsp;&nbsp;&nbsp;print('Title')<br></p></p></p></div>

  <hr>
  <div class="col-md-8 mb-2">
  <h2 class="types_text" style="color:#007bff; font-style:italic; text-align:left;">웹크롤링 링크 가져오기</h2><br><br>
   
  <p class="bd-yellow-100 p-3 ms-3"> 
  -------------- 위키백과에서 케빈베이컨 관련 카테고리 가져오기 --------------  <br>
  from urllib.request import urlopen<br>
  from bs4 import BeautifulSoup<br>
  import re<br><br>
  html = urlopen('http://en.wikipedia.org/wiki/kevin_Bacon')<br>
  bs = BeautifulSoup(html, 'html.parser')<br>
  for link in bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$')):<br>
      if 'href' in link.attrs:<br>
          print(link.attrs['href'])<br>
  -------------- URL을 받아 링크목록을 반환하고 그 링크목록 연관 링크목록 가져오기 --------------  <br>
  from urllib.request import urlopen<br>
  from bs4 import BeautifulSoup<br>
  import datetime <br>
  import random<br>
  import re<br><br>
  random.seed(datetime.datetime.now())<br><br>
  def getLink(articleURL):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;html = urlopen('http://en.wikipedia.org{}.format(aricleURL))<br>
  &nbsp;&nbsp;&nbsp;&nbsp;bs = BeautifulSoup(html, 'html.parser')<br>
  &nbsp;&nbsp;&nbsp;&nbsp;return bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))<br>
  Links = getLink('wiki/kevin_Bacon')<br> 
  while len(Links) > 0 :<br> 
  &nbsp;&nbsp;&nbsp;&nbsp;newArticle = links[random.radint(0, len(Links)-1)].attrs['href']  <br> 
  &nbsp;&nbsp;&nbsp;&nbsp;print(newArticle)<br> 
  &nbsp;&nbsp;&nbsp;&nbsp;links = getLinks(newArticle)<br> 
  from urllib.request import urlopen <br>
  from bs4 import BeautifulSoup <br>
  import datetime <br>
  import random<br>
  import re<br><br>
  random.seed(datetime.datetime.now())<br><br>

  def getLinks(articleURL):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;html = urlopen('http://en.wikipedia.org{}'.format(articleURL))<br>
  &nbsp;&nbsp;&nbsp;&nbsp;bs = BeautifulSoup(html, 'html.parser')<br>
  &nbsp;&nbsp;&nbsp;&nbsp;return bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))<br>
  Links = getLinks('/wiki/kevin_Bacon')<br>
  while len(Links) > 0 :<br>
  &nbsp;&nbsp;&nbsp;&nbsp;newArticle = Links[random.randint(0, len(Links)-1)].attrs['href']<br>  
  &nbsp;&nbsp;&nbsp;&nbsp;print(newArticle)<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Links = getLinks(newArticle)<br></p>
  <p>위 예제에서 링크목록에서 다음 링크목록을 가져올때 중복없이 유일한 요소를 저장하기 위해 set()이 사용된다</p>

  <p class="bd-yellow-100 p-3 ms-3"> 
  -------------- 위키백과 케빈베이컨 링크 중복없이 가져오기 --------------  <br>
  from urllib.request import urlopen  <br>
  from bs4 import BeautifulSoup  <br>
  import datetime  <br>
  import random <br>
  import re <br> <br>

  pages = set() <br>
  random.seed(datetime.datetime.now()) <br> <br>

  def getInternalLinks(bs, includeURL): <br>
  &nbsp;&nbsp;&nbsp;&nbsp;includeURL = urlopen('{}:{}'.format(urlparse(includeURL).scheme, urlparse(includeURL).netloc))<br>
  &nbsp;&nbsp;&nbsp;&nbsp;internalLinks =[]<br> 
  &nbsp;&nbsp;&nbsp;&nbsp;for link in bs.findAll('a', href=re.compile('^/|.*' + includeURL + ')')<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if link.attrs['href'] is not None :<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if link.attrs['href'] is not in internalLinks:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (link.attrs['href'].startswith('/')):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;internalLinks.append(includeURL + link.attrs['href'])<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:            <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;internalLinks.append(link.attrs['href'])<br>
  &nbsp;&nbsp;&nbsp;&nbsp;return internalLinks<br><br>

  def getExternalLinks(bs, excludeURL): <br>
  &nbsp;&nbsp;&nbsp;&nbsp;excludeURL = urlopen('{}:{}'.format(urlparse(excludeURL).scheme, urlparse(excludeURL).netloc))<br>
  &nbsp;&nbsp;&nbsp;&nbsp;exernalLinks =[]<br> 
  &nbsp;&nbsp;&nbsp;&nbsp;for link in bs.findAll('a', href=re.compile('^/|.*' + excludeURL + ')')<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if link.attrs['href'] is not None :<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if link.attrs['href'] is not in externalLinks:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if (link.attrs['href'].startswith('/')):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;externalLinks.append(excludeURL + link.attrs['href'])<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:            <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;externalLinks.append(link.attrs['href'])<br>        
  &nbsp;&nbsp;&nbsp;&nbsp;return externalLinks<br><br>

  def getRandomExternalLink(startingPage) :<br>
  &nbsp;&nbsp;&nbsp;&nbsp;html = urlopen(startingPage)<br>
  &nbsp;&nbsp;&nbsp;&nbsp;bs = BeautifulSoup(html, 'html.parser') <br>
  &nbsp;&nbsp;&nbsp;&nbsp;exernalLinks = getExternalLinks(bs, urlparse(startingPage).netloc)  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;if len(externalLinks) == 0: <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print('No external Links, looking around the site for one')<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;domain = '{}:{}'.format(urlparse(startingPage).scheme, urlparse(startingPage).netloc)<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;internalLinks = getInternalLinks(bs, domain)<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return getRandomExternalLink(internalLinks[random.randint(0,len(internalLinks)-1)]) <br>
  &nbsp;&nbsp;&nbsp;&nbsp;else:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return getRandomExternalLink(externalLinks[random.randint(0,len(externalLinks)-1)]) <br><br>
           
  def followExternalOnly(startingSite):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;externalLink = getRandomExternalLink(startingSite):<br>
  &nbsp;&nbsp;&nbsp;&nbsp;print('Random external link is : {}'.format(externalLink))<br>
  &nbsp;&nbsp;&nbsp;&nbsp;followExternalOnly(externalLink)<br><br>

  followExternalOnly('http://oreilly.com')  <br>
 </p></p></p></div><br>
</div>
    
</body>
</html>